{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stream():\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        self.file = open(filename, 'r')\n",
    "        \n",
    "        self.line = 1\n",
    "        self.position = 0\n",
    "        \n",
    "    def get(self):\n",
    "        c = self.file.read(1)\n",
    "        if c:\n",
    "            if c == '\\n':\n",
    "                self.line += 1\n",
    "                self.position = 0\n",
    "            else:\n",
    "                self.position += 1\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "\n",
    "    T_IDENTIFIER = 0\n",
    "    T_KEYWORD = 1\n",
    "    T_SEPARATOR = 2\n",
    "    T_EOF = 3\n",
    "    T_NEWLINE = 4\n",
    "    \n",
    "    # identifiers:\n",
    "    # keywords: id, dom, cod, cat\n",
    "    # separators: (, ), =, =>, ->, ~>,  \n",
    "\n",
    "    def __init__(self, T, line, position, data = ''):\n",
    "        self.type = T\n",
    "        self.line = line\n",
    "        self.position = position\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer():\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.stream = stream\n",
    "        self.current_token = None\n",
    "        self.tmp = ''\n",
    "        self.tmp_line = 1\n",
    "        self.tmp_position = 0\n",
    "        \n",
    "    def tokenize(self, expr):\n",
    "        if expr in ['id', 'dom', 'cod', 'cat', 'let', 'property', 'theorem', 'given', 'with', 'then', 'exists']:\n",
    "            return Token(Token.T_KEYWORD, self.tmp_line, self.tmp_position, expr)\n",
    "\n",
    "        if expr in ['(', ')', '{', '}', '=', '.', ',', '=>', ':', '->', '~>']:\n",
    "            return Token(Token.T_SEPARATOR, self.tmp_line, self.tmp_position, expr)\n",
    "        \n",
    "        if expr in ['\\n']:\n",
    "            return Token(Token.T_NEWLINE, self.tmp_line, self.tmp_position)\n",
    "        \n",
    "        if re.match(r'\\A\\w+\\Z', expr):\n",
    "            return Token(Token.T_IDENTIFIER, self.tmp_line, self.tmp_position, expr)\n",
    "                    \n",
    "        return None\n",
    "    \n",
    "    def get_token(self):\n",
    "        # Read characters until a new Token is produced\n",
    "        while True:\n",
    "            c = stream.get()\n",
    "\n",
    "            # End of file\n",
    "            if not c:\n",
    "                if self.tmp == '':\n",
    "                    return Token(Token.T_EOF, -1, -1, '?') # TODO\n",
    "                    \n",
    "                token = self.tokenize(self.tmp)\n",
    "                if not token:\n",
    "                    raise Exception('Unexpected token \\'{}\\''.format(self.tmp))\n",
    "                self.current_token = None\n",
    "                self.tmp = ''\n",
    "                return token\n",
    "            \n",
    "            # Whitespace: always marks the end of a token (if there currently is one)\n",
    "            if c in [' ', '\\t']:\n",
    "                if self.tmp == '':\n",
    "                    continue\n",
    "\n",
    "                if not self.current_token:\n",
    "                    raise Exception('Unknown token \\'{}\\''.format(self.tmp))\n",
    "                \n",
    "                token = self.current_token\n",
    "                self.current_token = None\n",
    "                self.tmp = ''\n",
    "                return token\n",
    "            \n",
    "            # Comments: marks the end of a token (if there currently is one),\n",
    "            # then continue discarding characters until a newline appears\n",
    "            if c in ['#']:\n",
    "                token = None\n",
    "                \n",
    "                if self.tmp != '':\n",
    "                    if not self.current_token:\n",
    "                        raise Exception('Unknown token \\'{}\\''.format(self.tmp))\n",
    "                    token = self.current_token\n",
    "                \n",
    "                while stream.get() != '\\n':\n",
    "                    pass\n",
    "                \n",
    "                self.tmp = '\\n'\n",
    "                self.tmp_line = self.stream.line\n",
    "                self.tmp_position = self.stream.position\n",
    "                self.current_token = self.tokenize(self.tmp)\n",
    "                \n",
    "                return token if token else self.get_token()\n",
    "            \n",
    "            # Try to enlarge the token if possible\n",
    "            token = self.tokenize(self.tmp + c)\n",
    "            if token:\n",
    "                self.current_token = token\n",
    "                if self.tmp == '':\n",
    "                    self.tmp_line = self.stream.line\n",
    "                    self.tmp_position = self.stream.position\n",
    "                self.tmp += c\n",
    "                continue\n",
    "\n",
    "            # If we also did not succeed before, hope that it will make sense later\n",
    "            if not self.current_token:\n",
    "                self.tmp += c\n",
    "                continue\n",
    "\n",
    "            # Return the last valid token\n",
    "            token = self.current_token\n",
    "            self.tmp = c\n",
    "            self.tmp_line = self.stream.line\n",
    "            self.tmp_position = self.stream.position\n",
    "            self.current_token = self.tokenize(self.tmp)\n",
    "            return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \n",
    "    def __init__(self, lexer):\n",
    "        self.lexer = lexer\n",
    "        self.current_token = None\n",
    "        \n",
    "        self.book = None\n",
    "    \n",
    "    def next_token(self):\n",
    "        self.current_token = lexer.get_token()\n",
    "        \n",
    "        if self.current_token.type == Token.T_NEWLINE:\n",
    "            self.next_token()\n",
    "    \n",
    "    def found(self, token_type, data = None):\n",
    "        return self.current_token.type == token_type and (data == None or data == self.current_token.data)\n",
    "        \n",
    "    def consume(self, token_type = None, data = None):            \n",
    "        if token_type == None or self.found(token_type, data):\n",
    "            token = self.current_token\n",
    "            self.next_token()\n",
    "            return token\n",
    "        else:\n",
    "            raise Exception('Expected to find token of type {} but found {} instead ({})'.format(token_type, self.current_token.type, self.current_token.data))\n",
    "    \n",
    "    def parse(self):\n",
    "        self.next_token()\n",
    "        \n",
    "        while not self.found(Token.T_EOF):\n",
    "            self.parse_statement(self.book)\n",
    "        \n",
    "        self.consume(Token.T_EOF)\n",
    "        \n",
    "    # ----------------------------------------------------------------\n",
    "        \n",
    "    def parse_statement(self, book):\n",
    "        # STATEMENT = \n",
    "        #  \\n |\n",
    "        #  let LIST_OF_IDENTIFIERS : TYPE |\n",
    "        #  property IDENTIFIER { GIVENS CONDITIONS }\n",
    "                \n",
    "        if self.found(Token.T_KEYWORD, 'let'):\n",
    "            self.consume()\n",
    "            identifiers = self.parse_list_of_identifiers()\n",
    "            self.consume(Token.T_SEPARATOR, ':')\n",
    "            m_type = self.parse_type(book)\n",
    "\n",
    "            if len(m_type) == 1: # Object\n",
    "                for i in identifiers:\n",
    "                    book.create_object(m_type[0], i)\n",
    "            else: # Morphism\n",
    "                for i in identifiers:\n",
    "                    book.create_morphism(m_type[0], m_type[1], i, covariant = m_type[2])                \n",
    "            \n",
    "            return True\n",
    "        \n",
    "        if self.found(Token.T_KEYWORD, 'property'):\n",
    "            self.consume()\n",
    "            name = self.consume(Token.T_IDENTIFIER).data\n",
    "            if book.has_property(name) or book.has_symbol(name):\n",
    "                raise Exception('Name \\'{}\\' is already used'.format(name))\n",
    "            prop = Property(name)\n",
    "            prop.add_reference(book)\n",
    "            \n",
    "            self.consume(Token.T_SEPARATOR, '{')\n",
    "            self.parse_givens(prop)\n",
    "            self.parse_conditions(prop)\n",
    "            self.consume(Token.T_SEPARATOR, '}')\n",
    "            \n",
    "            book.add_property(name, prop)\n",
    "            return True\n",
    "        \n",
    "        if self.found(Token.T_KEYWORD, 'theorem'):\n",
    "            self.consume()\n",
    "            name = self.consume(Token.T_IDENTIFIER).data\n",
    "            if book.has_theorem(name):\n",
    "                raise Exception('There is already a theorem named \\'{}\\''.format(name))\n",
    "            thm = Theorem()\n",
    "            thm.add_reference(book)\n",
    "            \n",
    "            self.consume(Token.T_SEPARATOR, '{')\n",
    "            self.parse_givens(thm)\n",
    "            self.parse_conditions(thm)\n",
    "            self.parse_conclusions(thm.conclusion)\n",
    "            self.consume(Token.T_SEPARATOR, '}')\n",
    "            \n",
    "            book.add_theorem(name, thm)\n",
    "            return True\n",
    "        \n",
    "        raise Exception('Unknown statement at line {} position {}'.format(self.current_token.line, self.current_token.position))        \n",
    "        return False\n",
    "    \n",
    "    def parse_givens(self, context):\n",
    "        # GIVENS =\n",
    "        #   GIVEN { GIVEN }\n",
    "        # \n",
    "        # GIVEN =\n",
    "        #   given LIST_OF_IDENTIFIERS : TYPE\n",
    "\n",
    "        if not self.found(Token.T_KEYWORD, 'given'):\n",
    "            raise Exception('Property must have some data')\n",
    "        \n",
    "        while self.found(Token.T_KEYWORD, 'given'):\n",
    "            self.consume()\n",
    "            identifiers = self.parse_list_of_identifiers()\n",
    "            self.consume(Token.T_SEPARATOR, ':')\n",
    "            m_type = self.parse_type(context)\n",
    "\n",
    "            if len(m_type) == 1: # Object\n",
    "                for i in identifiers:\n",
    "                    x = context.create_object(m_type[0], i)\n",
    "                    context.add_data(x)\n",
    "            else: # Morphism\n",
    "                for i in identifiers:\n",
    "                    x = context.create_morphism(m_type[0], m_type[1], i, covariant = m_type[2])\n",
    "                    context.add_data(x)\n",
    "    \n",
    "    def parse_conditions(self, context):\n",
    "        # CONDITIONS =\n",
    "        #   CONDITION { CONDITION }\n",
    "        # \n",
    "        # CONDITION =\n",
    "        #   with OBJECT\n",
    "        \n",
    "        while self.found(Token.T_KEYWORD, 'with'):\n",
    "            self.consume()\n",
    "            C = self.parse_object(context)\n",
    "            if C.category != Cat:\n",
    "                raise Exception('Condition must be a category!')\n",
    "                \n",
    "            context.add_condition(C)\n",
    "    \n",
    "    def parse_conclusions(self, conclusion):\n",
    "        # CONCLUSIONS =\n",
    "        #   CONCLUSION { CONCLUSION }\n",
    "        # \n",
    "        # CONCLUSION =\n",
    "        #   then exists LIST_OF_IDENTIFIERS : TYPE |\n",
    "        #   then OBJECT\n",
    "        \n",
    "        while self.found(Token.T_KEYWORD, 'then'):\n",
    "            self.consume()\n",
    "            \n",
    "            if self.found(Token.T_KEYWORD, 'exists'):\n",
    "                self.consume()\n",
    "                identifiers = self.parse_list_of_identifiers()\n",
    "                self.consume(Token.T_SEPARATOR, ':')\n",
    "                m_type = self.parse_type(conclusion)\n",
    "                if len(m_type) == 1: # Object\n",
    "                    for i in identifiers:\n",
    "                        x = conclusion.create_object(m_type[0], i)\n",
    "                else: # Morphism\n",
    "                    for i in identifiers:\n",
    "                        x = conclusion.create_morphism(m_type[0], m_type[1], i, covariant = m_type[2])\n",
    "            \n",
    "            else:\n",
    "                C = self.parse_object(conclusion)\n",
    "                if C.category != Cat:\n",
    "                    raise Exception('Conclusion must be a category!')\n",
    "                \n",
    "                conclusion.create_object(C)\n",
    "            \n",
    "    def parse_list_of_identifiers(self):\n",
    "        # LIST_OF_IDENTIFIERS =\n",
    "        #   IDENTIFIER { , IDENTIFIER }\n",
    "        \n",
    "        identifiers = []\n",
    "        identifiers.append(self.consume(Token.T_IDENTIFIER).data)\n",
    "        \n",
    "        while self.found(Token.T_SEPARATOR, ','):\n",
    "            self.consume()\n",
    "            identifiers.append(self.consume(Token.T_IDENTIFIER).data)\n",
    "        \n",
    "        return identifiers\n",
    "    \n",
    "    def parse_type(self, diagram):\n",
    "        # TYPE =\n",
    "        #   OBJECT |\n",
    "        #   OBJECT -> OBJECT |\n",
    "        #   OBJECT ~> OBJECT\n",
    "        \n",
    "        X = self.parse_object(diagram)\n",
    "        \n",
    "        is_arrow = False\n",
    "        if self.found(Token.T_SEPARATOR, '->'):\n",
    "            is_arrow = True\n",
    "            covariant = True\n",
    "            \n",
    "        if self.found(Token.T_SEPARATOR, '~>'):\n",
    "            is_arrow = True\n",
    "            covariant = False\n",
    "        \n",
    "        if is_arrow:\n",
    "            self.consume()\n",
    "            Y = self.parse_object(diagram)\n",
    "            return (X, Y, covariant)\n",
    "        else:\n",
    "            return (X,)\n",
    "        \n",
    "    def parse_object(self, diagram):\n",
    "        # OBJECT = \n",
    "        #   ( OBJECT ) |\n",
    "        #   id ( OBJECT ) |\n",
    "        #   dom ( OBJECT ) |\n",
    "        #   cod ( OBJECT ) |\n",
    "        #   cat ( OBJECT ) |\n",
    "        #   IDENTIFIER ( LIST_OF_OBJECTS ) |\n",
    "        #   IDENTIFIER |\n",
    "        #   OBJECT = OBJECT |\n",
    "        #   OBJECT => OBJECT | TODO\n",
    "        #   OBJECT & OBJECT | TODO\n",
    "        #   OBJECT + OBJECT | TODO\n",
    "        #   ... more?\n",
    "        \n",
    "        x = None\n",
    "        \n",
    "        if self.found(Token.T_SEPARATOR, '('):\n",
    "            self.consume()\n",
    "            x = self.parse_object(diagram)\n",
    "            self.consume(Token.T_SEPARATOR, ')')\n",
    "        \n",
    "        elif self.found(Token.T_KEYWORD, 'id'):\n",
    "            self.consume()\n",
    "            self.consume(Token.T_SEPARATOR, '(')\n",
    "            x = self.parse_object(diagram)\n",
    "            if not isinstance(x, Object):\n",
    "                raise Exception('id can only be appled to objects!') # TODO: can it?\n",
    "            self.consume(Token.T_SEPARATOR, ')')\n",
    "            \n",
    "        elif self.found(Token.T_KEYWORD, 'dom'):\n",
    "            self.consume()\n",
    "            self.consume(Token.T_SEPARATOR, '(')\n",
    "            x = self.parse_object(diagram).domain\n",
    "            self.consume(Token.T_SEPARATOR, ')')\n",
    "            \n",
    "        elif self.found(Token.T_KEYWORD, 'cod'):\n",
    "            self.consume()\n",
    "            self.consume(Token.T_SEPARATOR, '(')\n",
    "            x = self.parse_object(diagram).codomain\n",
    "            self.consume(Token.T_SEPARATOR, ')')\n",
    "\n",
    "        elif self.found(Token.T_KEYWORD, 'cat'):\n",
    "            self.consume()\n",
    "            self.consume(Token.T_SEPARATOR, '(')\n",
    "            x = self.parse_object(diagram).category\n",
    "            self.consume(Token.T_SEPARATOR, ')')\n",
    "            \n",
    "        elif self.found(Token.T_IDENTIFIER):\n",
    "            identifier_token = self.consume()\n",
    "            name = identifier_token.data\n",
    "            \n",
    "            if self.found(Token.T_SEPARATOR, '('):\n",
    "                self.consume()\n",
    "                objects = self.parse_list_of_objects(diagram)\n",
    "                self.consume(Token.T_SEPARATOR, ')')\n",
    "                x = diagram.apply(name, objects)\n",
    "                if x == None:\n",
    "                    raise Exception('Cannot apply {} to {}'.format(name, objects))\n",
    "            else:\n",
    "                x = diagram.find_morphism(name)\n",
    "                if x == None:\n",
    "                    raise Exception('Unknown identifier \\'{}\\' at ...'.format(name))\n",
    "        \n",
    "        if x == None:\n",
    "            raise Exception('Expected an object at line {} position {}'.format(self.current_token.line, self.current_token.position))\n",
    "        \n",
    "        # Now we have some x, see if we can (possibly) extend it!\n",
    "        # This seems to be the solution to left-recursive patterns\n",
    "        while True:\n",
    "            \n",
    "            if self.found(Token.T_SEPARATOR, '='):\n",
    "                self.consume()\n",
    "                y = self.parse_object(diagram)\n",
    "                x = diagram.create_equality(x, y)\n",
    "                continue\n",
    "            \n",
    "            if self.found(Token.T_SEPARATOR, '=>'):\n",
    "                pass\n",
    "            \n",
    "            break\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    def parse_list_of_objects(self, diagram):\n",
    "        # LIST_OF_OBJECTS =\n",
    "        #   OBJECT { , OBJECT }\n",
    "        \n",
    "        objects = []\n",
    "        objects.append(self.parse_object(diagram))\n",
    "        \n",
    "        while self.found(Token.T_SEPARATOR, ','):\n",
    "            self.consume()\n",
    "            objects.append(self.parse_object(diagram))\n",
    "        \n",
    "        return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = recreate_global_diagram()\n",
    "\n",
    "stream = Stream('/Users/jessetvogel/Projects/abstract-nonsense/math/small_test.txt')\n",
    "lexer = Lexer(stream)\n",
    "parser = Parser(lexer)\n",
    "\n",
    "parser.book = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ X }'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(G.properties['affine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ X, affine(X) }'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(G.theorems['my_thm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
