{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from core import *\n",
    "from prover import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stream():\n",
    "    \n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.line = 1\n",
    "        self.position = 0\n",
    "        \n",
    "    def get(self):\n",
    "        c = self.file.read(1)\n",
    "        if c:\n",
    "            if c == '\\n':\n",
    "                self.line += 1\n",
    "                self.position = 0\n",
    "            else:\n",
    "                self.position += 1\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "\n",
    "    T_IDENTIFIER = 0\n",
    "    T_KEYWORD = 1\n",
    "    T_SEPARATOR = 2\n",
    "    T_EOF = 3\n",
    "    T_NEWLINE = 4\n",
    "    T_NUMBER = 5\n",
    "    \n",
    "    # identifiers:\n",
    "    # keywords: id, dom, cod, cat\n",
    "    # separators: (, ), =, =>, ->, ~>,  \n",
    "\n",
    "    def __init__(self, T, line, position, data = ''):\n",
    "        self.type = T\n",
    "        self.line = line\n",
    "        self.position = position\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexer():\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.stream = stream\n",
    "        self.current_token = None\n",
    "        self.tmp = ''\n",
    "        self.tmp_line = 1\n",
    "        self.tmp_position = 0\n",
    "        \n",
    "    def tokenize(self, expr):\n",
    "        if expr in ['id', 'dom', 'cod', 'cat', 'let', 'assume', 'prove', 'property', 'theorem', 'given', 'with', 'then', 'exists', 'whats']:\n",
    "            return Token(Token.T_KEYWORD, self.tmp_line, self.tmp_position, expr)\n",
    "\n",
    "        if expr in ['(', ')', '{', '}', '=', '.', ',', ':', ';', '->', '~>', '=>']:\n",
    "            return Token(Token.T_SEPARATOR, self.tmp_line, self.tmp_position, expr)\n",
    "        \n",
    "        if expr in ['\\n']:\n",
    "            return Token(Token.T_NEWLINE, self.tmp_line, self.tmp_position)\n",
    "        \n",
    "        if re.match(r'\\A\\d+\\Z', expr):\n",
    "            return Token(Token.T_NUMBER, self.tmp_line, self.tmp_position, expr)\n",
    "        \n",
    "        if re.match(r'\\A\\w+\\Z', expr):\n",
    "            return Token(Token.T_IDENTIFIER, self.tmp_line, self.tmp_position, expr)\n",
    "                    \n",
    "        return None\n",
    "    \n",
    "    def get_token(self):\n",
    "        # Read characters until a new Token is produced\n",
    "        while True:\n",
    "            c = self.stream.get()\n",
    "\n",
    "            # End of file\n",
    "            if not c:\n",
    "                if self.tmp == '':\n",
    "                    return Token(Token.T_EOF, -1, -1, '?') # TODO\n",
    "                    \n",
    "                token = self.tokenize(self.tmp)\n",
    "                if not token:\n",
    "                    raise Exception('Unexpected token \\'{}\\''.format(self.tmp))\n",
    "                self.current_token = None\n",
    "                self.tmp = ''\n",
    "                return token\n",
    "            \n",
    "            # Whitespace: always marks the end of a token (if there currently is one)\n",
    "            if c in [' ', '\\t']:\n",
    "                if self.tmp == '':\n",
    "                    continue\n",
    "\n",
    "                if not self.current_token:\n",
    "                    raise Exception('Unknown token \\'{}\\''.format(self.tmp))\n",
    "                \n",
    "                token = self.current_token\n",
    "                self.current_token = None\n",
    "                self.tmp = ''\n",
    "                return token\n",
    "            \n",
    "            # Comments: marks the end of a token (if there currently is one),\n",
    "            # then continue discarding characters until a newline appears\n",
    "            if c in ['#']:\n",
    "                token = None\n",
    "                \n",
    "                if self.tmp != '':\n",
    "                    if not self.current_token:\n",
    "                        raise Exception('Unknown token \\'{}\\''.format(self.tmp))\n",
    "                    token = self.current_token\n",
    "                \n",
    "                while self.stream.get() != '\\n':\n",
    "                    pass\n",
    "                \n",
    "                self.tmp = '\\n'\n",
    "                self.tmp_line = self.stream.line\n",
    "                self.tmp_position = self.stream.position\n",
    "                self.current_token = self.tokenize(self.tmp)\n",
    "                \n",
    "                return token if token else self.get_token()\n",
    "            \n",
    "            # Try to enlarge the token if possible\n",
    "            token = self.tokenize(self.tmp + c)\n",
    "            if token:\n",
    "                self.current_token = token\n",
    "                if self.tmp == '':\n",
    "                    self.tmp_line = self.stream.line\n",
    "                    self.tmp_position = self.stream.position\n",
    "                self.tmp += c\n",
    "                continue\n",
    "\n",
    "            # If we also did not succeed before, hope that it will make sense later\n",
    "            if not self.current_token:\n",
    "                self.tmp += c\n",
    "                continue\n",
    "\n",
    "            # Return the last valid token\n",
    "            token = self.current_token\n",
    "            self.tmp = c\n",
    "            self.tmp_line = self.stream.line\n",
    "            self.tmp_position = self.stream.position\n",
    "            self.current_token = self.tokenize(self.tmp)\n",
    "            return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsingError(Exception):\n",
    "    \n",
    "    def __init__(self, message, token):\n",
    "        self.message = message\n",
    "        self.token = token\n",
    "        \n",
    "class InterpretationError(Exception):\n",
    "    \n",
    "    def __init__(self, message, token):\n",
    "        self.message = message\n",
    "        self.token = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \n",
    "    def __init__(self, lexer):\n",
    "        self.lexer = lexer\n",
    "        self.current_token = None\n",
    "        \n",
    "        self.book = None\n",
    "    \n",
    "    def next_token(self):\n",
    "        self.current_token = self.lexer.get_token()\n",
    "        \n",
    "        if self.current_token.type == Token.T_NEWLINE:\n",
    "            self.next_token()\n",
    "    \n",
    "    def found(self, token_type, data = None):\n",
    "        return self.current_token.type == token_type and (data == None or data == self.current_token.data)\n",
    "        \n",
    "    def consume(self, token_type = None, data = None):            \n",
    "        if token_type == None or self.found(token_type, data):\n",
    "            token = self.current_token\n",
    "            self.next_token()\n",
    "            return token\n",
    "        else:\n",
    "            raise ParsingError('Expected \\'{}\\' but found \\'{}\\''.format(data, self.current_token.data), self.current_token)\n",
    "    \n",
    "    def parse(self):\n",
    "        self.next_token()\n",
    "        \n",
    "        while not self.found(Token.T_EOF):\n",
    "            self.parse_statement(self.book)\n",
    "        \n",
    "        self.consume(Token.T_EOF)\n",
    "        \n",
    "    # ----------------------------------------------------------------\n",
    "        \n",
    "    def parse_statement(self, book):\n",
    "        # STATEMENT = \n",
    "        #  ; |\n",
    "        #  let LIST_OF_IDENTIFIERS : TYPE |\n",
    "        #  assume OBJECT |\n",
    "        #  prove OBJECT |\n",
    "        #  property IDENTIFIER { GIVENS CONDITIONS } |\n",
    "        #  theorem IDENTIFIER { GIVENS CONDITIONS CONCLUSIONS } |\n",
    "        #\n",
    "        #  whats OBJECT\n",
    "               \n",
    "        if self.found(Token.T_SEPARATOR, ';'):\n",
    "            self.consume()\n",
    "            return True\n",
    "            \n",
    "        if self.found(Token.T_KEYWORD, 'let'):\n",
    "            self.consume()\n",
    "            identifiers = self.parse_list_of_identifiers()\n",
    "            self.consume(Token.T_SEPARATOR, ':')\n",
    "            m_type = self.parse_type(book)\n",
    "\n",
    "            if len(m_type) == 1: # Object\n",
    "                for i in identifiers:\n",
    "                    book.create_object(m_type[0], i)\n",
    "            else: # Morphism\n",
    "                for i in identifiers:\n",
    "                    book.create_morphism(m_type[0], m_type[1], i, covariant = m_type[2])                \n",
    "            \n",
    "            return True\n",
    "        \n",
    "        if self.found(Token.T_KEYWORD, 'assume'):\n",
    "            self.consume()\n",
    "            t = self.current_token\n",
    "            C = self.parse_object(book)\n",
    "            if not C.is_category():\n",
    "                raise InterpretationError('Assume requires a category!', t)\n",
    "            book.create_object(C)\n",
    "            return True\n",
    "        \n",
    "        if self.found(Token.T_KEYWORD, 'prove'):\n",
    "            self.consume()\n",
    "            t = self.current_token\n",
    "            C = self.parse_object(book)\n",
    "            if not C.is_category():\n",
    "                raise InterpretationError('Prove requires a category!', t)\n",
    "            prover = Prover(book, C)\n",
    "            return prover.prove()\n",
    "        \n",
    "        if self.found(Token.T_KEYWORD, 'property'):\n",
    "            self.consume()\n",
    "            t_identifier = self.consume(Token.T_IDENTIFIER)\n",
    "            name = t_identifier.data\n",
    "            if book.has_property(name) or book.has_symbol(name):\n",
    "                raise InterpretationError('Name \\'{}\\' already used'.format(name), t_identifier)\n",
    "            prop = Property(name)\n",
    "            prop.add_reference(book)\n",
    "            \n",
    "            self.consume(Token.T_SEPARATOR, '{')\n",
    "            self.parse_givens(prop)\n",
    "            self.parse_conditions(prop)\n",
    "            self.consume(Token.T_SEPARATOR, '}')\n",
    "            \n",
    "            book.add_property(name, prop)\n",
    "            return True\n",
    "        \n",
    "        if self.found(Token.T_KEYWORD, 'theorem'):\n",
    "            self.consume()\n",
    "            t_identifier = self.consume(Token.T_IDENTIFIER)\n",
    "            name = t_identifier.data\n",
    "            if book.has_theorem(name):\n",
    "                raise InterpretationError('Name \\'{}\\' already used'.format(name), t_identifier)\n",
    "            thm = Theorem()\n",
    "            thm.add_reference(book)\n",
    "            \n",
    "            self.consume(Token.T_SEPARATOR, '{')\n",
    "            self.parse_givens(thm)\n",
    "            self.parse_conditions(thm)\n",
    "            self.parse_conclusions(thm.conclusion)\n",
    "            self.consume(Token.T_SEPARATOR, '}')\n",
    "            \n",
    "            book.add_theorem(name, thm)\n",
    "            return True\n",
    "        \n",
    "        if self.found(Token.T_KEYWORD, 'whats'):\n",
    "            self.consume()\n",
    "            t = self.current_token\n",
    "            x = self.parse_object(book)\n",
    "            if x.is_object():\n",
    "                print('{} : {}'.format(book.str_x(x), book.str_x(x.category)))\n",
    "            else:\n",
    "                covariant = x.covariant if x.is_functor() else True\n",
    "                print('{} : {} {} {}'.format(book.str_x(x), book.str_x(x.domain), '->' if covariant else '~>', book.str_x(x.codomain)))\n",
    "            \n",
    "            return True            \n",
    "        \n",
    "        raise ParsingError('Unable to parse statement', self.current_token)\n",
    "    \n",
    "    def parse_givens(self, context):\n",
    "        # GIVENS =\n",
    "        #   GIVEN { GIVEN }\n",
    "        # \n",
    "        # GIVEN =\n",
    "        #   given LIST_OF_IDENTIFIERS : TYPE\n",
    "\n",
    "        if not self.found(Token.T_KEYWORD, 'given'):\n",
    "            raise ParsingError('Expected \\'given\\': property must have some data', self.current_token)\n",
    "        \n",
    "        while self.found(Token.T_KEYWORD, 'given'):\n",
    "            t_given = self.consume()\n",
    "            identifiers = self.parse_list_of_identifiers()\n",
    "            for i in identifiers:\n",
    "                if not context.is_name_available(i):\n",
    "                    raise InterpretationError('Name \\'{}\\' is already used', t_given)\n",
    "            \n",
    "            self.consume(Token.T_SEPARATOR, ':')\n",
    "            m_type = self.parse_type(context)\n",
    "\n",
    "            if len(m_type) == 1: # case Object\n",
    "                for i in identifiers:\n",
    "                    x = context.create_object(m_type[0], i)\n",
    "                    context.add_data(x)\n",
    "            else: # case Morphism\n",
    "                for i in identifiers:\n",
    "                    x = context.create_morphism(m_type[0], m_type[1], i, covariant = m_type[2])\n",
    "                    context.add_data(x)\n",
    "    \n",
    "    def parse_conditions(self, context):\n",
    "        # CONDITIONS =\n",
    "        #   CONDITION { CONDITION }\n",
    "        # \n",
    "        # CONDITION =\n",
    "        #   with OBJECT\n",
    "        \n",
    "        while self.found(Token.T_KEYWORD, 'with'):\n",
    "            self.consume()\n",
    "            t_condition = self.current_token\n",
    "            C = self.parse_object(context)\n",
    "            if not C.is_category():\n",
    "                raise InterpretationError('Condition must be a category!', t_condition)\n",
    "            \n",
    "            context.add_condition(C)\n",
    "    \n",
    "    def parse_conclusions(self, conclusion):\n",
    "        # CONCLUSIONS =\n",
    "        #   CONCLUSION { CONCLUSION }\n",
    "        # \n",
    "        # CONCLUSION =\n",
    "        #   then exists LIST_OF_IDENTIFIERS : TYPE |\n",
    "        #   then OBJECT\n",
    "        \n",
    "        while self.found(Token.T_KEYWORD, 'then'):\n",
    "            self.consume()\n",
    "            \n",
    "            if self.found(Token.T_KEYWORD, 'exists'):\n",
    "                self.consume()\n",
    "                identifiers = self.parse_list_of_identifiers()\n",
    "                self.consume(Token.T_SEPARATOR, ':')\n",
    "                m_type = self.parse_type(conclusion)\n",
    "                if len(m_type) == 1: # Object\n",
    "                    for i in identifiers:\n",
    "                        x = conclusion.create_object(m_type[0], i)\n",
    "                else: # Morphism\n",
    "                    for i in identifiers:\n",
    "                        x = conclusion.create_morphism(m_type[0], m_type[1], i, covariant = m_type[2])\n",
    "            \n",
    "            else:\n",
    "                C = self.parse_object(conclusion)\n",
    "                if not C.is_category():\n",
    "                    raise InterpretationError('Conclusion must be a category!')\n",
    "                \n",
    "                conclusion.create_object(C)\n",
    "            \n",
    "    def parse_list_of_identifiers(self):\n",
    "        # LIST_OF_IDENTIFIERS =\n",
    "        #   IDENTIFIER { , IDENTIFIER }\n",
    "        \n",
    "        identifiers = []\n",
    "        identifiers.append(self.consume(Token.T_IDENTIFIER).data)\n",
    "        \n",
    "        while self.found(Token.T_SEPARATOR, ','):\n",
    "            self.consume()\n",
    "            identifiers.append(self.consume(Token.T_IDENTIFIER).data)\n",
    "        \n",
    "        return identifiers\n",
    "    \n",
    "    def parse_type(self, diagram):\n",
    "        # TYPE =\n",
    "        #   OBJECT |\n",
    "        #   OBJECT -> OBJECT |\n",
    "        #   OBJECT ~> OBJECT\n",
    "        \n",
    "        X = self.parse_object(diagram)\n",
    "        \n",
    "        is_arrow = False\n",
    "        if self.found(Token.T_SEPARATOR, '->'):\n",
    "            is_arrow = True\n",
    "            covariant = True\n",
    "            \n",
    "        if self.found(Token.T_SEPARATOR, '~>'):\n",
    "            is_arrow = True\n",
    "            covariant = False\n",
    "        \n",
    "        if is_arrow:\n",
    "            self.consume()\n",
    "            Y = self.parse_object(diagram)\n",
    "            return (X, Y, covariant)\n",
    "        else:\n",
    "            return (X,)\n",
    "        \n",
    "    def parse_object(self, diagram):\n",
    "        # OBJECT = \n",
    "        #   ( OBJECT ) |\n",
    "        #   id ( OBJECT ) |\n",
    "        #   dom ( OBJECT ) |\n",
    "        #   cod ( OBJECT ) |\n",
    "        #   cat ( OBJECT ) |\n",
    "        #   NUMBER |\n",
    "        #   IDENTIFIER ( LIST_OF_OBJECTS ) |\n",
    "        #   IDENTIFIER |\n",
    "        #   OBJECT ( LIST_OF_OBJECTS ) |\n",
    "        #   OBJECT = OBJECT |\n",
    "        #   OBJECT . OBJECT |\n",
    "        #   OBJECT => OBJECT | TODO\n",
    "        #   OBJECT & OBJECT | TODO\n",
    "        #   OBJECT + OBJECT | TODO\n",
    "        #   ... more?\n",
    "        \n",
    "        x = None\n",
    "        \n",
    "        if self.found(Token.T_SEPARATOR, '('):\n",
    "            self.consume()\n",
    "            x = self.parse_object(diagram)\n",
    "            self.consume(Token.T_SEPARATOR, ')')\n",
    "        \n",
    "        elif self.found(Token.T_KEYWORD, 'id'):\n",
    "            self.consume()\n",
    "            self.consume(Token.T_SEPARATOR, '(')\n",
    "            x = self.parse_object(diagram)\n",
    "            if not x.is_object():\n",
    "                raise InterpretationError('id can only be appled to objects!') # TODO: can it?\n",
    "            self.consume(Token.T_SEPARATOR, ')')\n",
    "            \n",
    "        elif self.found(Token.T_KEYWORD, 'dom'):\n",
    "            self.consume()\n",
    "            self.consume(Token.T_SEPARATOR, '(')\n",
    "            x = self.parse_object(diagram).domain\n",
    "            self.consume(Token.T_SEPARATOR, ')')\n",
    "            \n",
    "        elif self.found(Token.T_KEYWORD, 'cod'):\n",
    "            self.consume()\n",
    "            self.consume(Token.T_SEPARATOR, '(')\n",
    "            x = self.parse_object(diagram).codomain\n",
    "            self.consume(Token.T_SEPARATOR, ')')\n",
    "\n",
    "        elif self.found(Token.T_KEYWORD, 'cat'):\n",
    "            self.consume()\n",
    "            self.consume(Token.T_SEPARATOR, '(')\n",
    "            x = self.parse_object(diagram).category\n",
    "            self.consume(Token.T_SEPARATOR, ')')\n",
    "        \n",
    "        elif self.found(Token.T_NUMBER):\n",
    "            t_number = self.consume()\n",
    "            x = diagram.create_number(int(t_number.data))\n",
    "        \n",
    "        elif self.found(Token.T_IDENTIFIER):\n",
    "            t_identifier = self.consume()\n",
    "            name = t_identifier.data\n",
    "            \n",
    "            # If name refers to a property\n",
    "            prop = diagram.get_property(name)\n",
    "            if prop != None:\n",
    "                self.consume(Token.T_SEPARATOR, '(')\n",
    "                objects = self.parse_list_of_objects(diagram)\n",
    "                self.consume(Token.T_SEPARATOR, ')')\n",
    "                x = diagram.apply_property(prop, objects)\n",
    "                if x == None:\n",
    "                    raise InterpretationError('Failed to apply property {} to {}'.format(name, objects), t_identifier)\n",
    "            \n",
    "            # Otherwise find morphism by name\n",
    "            x = diagram.get_morphism(name)\n",
    "            if x == None:\n",
    "                raise InterpretationError('Unknown identifier \\'{}\\''.format(name), t_identifier)\n",
    "        \n",
    "        if x == None:\n",
    "            raise ParsingError('Expected an object/morphism', self.current_token)\n",
    "        \n",
    "        # Now we have some x, see if we can (possibly) extend it!\n",
    "        # This seems to be the solution to left-recursive patterns\n",
    "        while True:\n",
    "            \n",
    "            if self.found(Token.T_SEPARATOR, '('):\n",
    "                self.consume()\n",
    "                objects = self.parse_list_of_objects(diagram)\n",
    "                self.consume(Token.T_SEPARATOR, ')')\n",
    "                x = diagram.apply_functor(x, objects)\n",
    "                if x == None:\n",
    "                    raise InterpretationError('Failed to apply {} to {}'.format(name, objects), t_identifier)\n",
    "            else:\n",
    "\n",
    "            \n",
    "            if self.found(Token.T_SEPARATOR, '='):\n",
    "                self.consume()\n",
    "                y = self.parse_object(diagram)\n",
    "                x = diagram.create_equality(x, y)\n",
    "                continue\n",
    "            \n",
    "            if self.found(Token.T_SEPARATOR, '.'):\n",
    "                self.consume()\n",
    "                y = self.parse_object(diagram)\n",
    "                x = diagram.create_composition([ x, y ])\n",
    "                continue\n",
    "            \n",
    "            if self.found(Token.T_SEPARATOR, '=>'):\n",
    "                pass\n",
    "            \n",
    "            break\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    def parse_list_of_objects(self, diagram):\n",
    "        # LIST_OF_OBJECTS =\n",
    "        #   OBJECT { , OBJECT }\n",
    "        \n",
    "        objects = []\n",
    "        objects.append(self.parse_object(diagram))\n",
    "        \n",
    "        while self.found(Token.T_SEPARATOR, ','):\n",
    "            self.consume()\n",
    "            objects.append(self.parse_object(diagram))\n",
    "        \n",
    "        return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
